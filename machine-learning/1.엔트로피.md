### 1. 엔트로피(entropy) 
 - 참고링크 : [데이터 사이언스 스쿨: 엔트로피](https://datascienceschool.net/02%20mathematics/10.01%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.html#:~:text=%2D%3E-,**%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC(entropy)**%EB%8A%94%20%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC%EA%B0%80%20%EA%B0%80%EC%A7%80%EB%8A%94%20%EC%A0%95%EB%B3%B4,%EA%B2%BD%EC%9A%B0%EC%97%90%EB%8A%94%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EA%B0%80%20%EB%86%92%EC%95%84%EC%A7%84%EB%8B%A4.)
                              
 - 확률분포가 가지는 정보의 확신도 혹은 정보량을 수치로 표현한 것(물리적 개념과 차이가 있음)
 - 확률분포에서 특정한 값이 나올 확률이 높아지고, 나머지 값의 확률은 낮아진다면 엔트로피는 작아진다. => 확률밀도가 특정한 값에 몰려 있는 경우 
 - 반대로 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 낮아진다. => 여러가지 값에 골고루 퍼져있는 경우
 - 확률변수 Y가 이산확률변수, K는 Y가 가질 수 있는 클래스의 수, $p(y)$는 확률질량함수인 경우 아래와 같이 엔트로피가 정의됨

$$
  H[Y] = -\sum_{k=1}^{K} p(y_k) \textrm{  } log \textrm{  } p(y_k) 
$$

- Y가 연속확률변수인 경우 아래와 같이 정의됨

$$
  H[Y] = -\int_{-\infty}^{\infty} p(y) \textrm{  } log \textrm{  } p(y) \textrm{  } dy 
$$

### 2. 엔트로피 값의 범위
 - 최소값 : 0, 확률분포에서 하나의 값이 나올 확률이 1인 경우,
 - 최대값 : 1, uniform distribution 

### 참고
 - 엔트로피는 가변길이 인코딩과 관련이 있음, 정보량이 최소가 됨, 하지만 딥러닝에서 관심있는 주제는 아니여서 이정도만. 엔트로피 응용으로 볼 수 있음
 - 지니불순도: log계산 없이 엔트로피와 유사한 결과를 얻을 수 있음

$$
  G[Y] = \sum_{k=1}^{K} P(y_k)(1 - P(y_k))
$$

### 3. 엔트로피 최대화
 - 기댓값 0, 분산 $\sigma^2$이 주어졌을 때 엔트로피 $H[p(x)]$를 가장 크게 만드는 확률밀도함수 $p(x)$는 정규분포가 된다.
 - 확률밀도함수가 지켜야할 제한 조건은, 1) 확률밀도함수 총면적 = 1, 2) 기댓값은 0, 3) 분산은 $\sigma^2$
 - 최대화할 목적범함수(objective functional)은 엔트로피 이다

$$
  H[p(x)] = - \int_{-\infty}^{\infty} p(x) log p(x) dx
$$

 - 라그랑주 승수법으로 제약조건을 추가하고, 풀면(증명 생략), 정규분포가 도출됨
 - 따라서, 정규분포는 기댓값과 표준편차를 알고 있는 확률분포 중에서 가장 엔트로피가 크고 따라서 가장 정보가 적은 확률분포이다. 
 - 정규분포는 베이즈 추정에서 사실상의 무정보 사전확률분포로 사용되는 경우가 많다.
