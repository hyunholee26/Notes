Uncertainty quatification을 위한 지표의 하나로 Negative Log Likelihood가 사용된다는 것이 잘 이해가 되지 않아
다음 블로그를 일게되 었는데, 정리가 필요하다고 생각하여 아래와 같이 정리한다.
 - https://jaejunyoo.blogspot.com/2018/02/minimizing-negative-log-likelihood-in-kor.html
 - https://jaejunyoo.blogspot.com/2018/02/inimizing-negative-log-likelihood-in-kor-2.html
 - https://jaejunyoo.blogspot.com/2018/02/minimizing-negative-log-likelihood-in-kor-3.html

### 0.목적
 - 이 글은 최종적으로 neural network에서 minimizaing the negative log likelihood의 의미를 설명합니다.

### 1. 1편에서는 아래의 개념들을 간단히 설명합니다.
  - Random variable, Probability distribution
  - (정리차원에서), probability와 likelihood에 대한 설명을 덧붙이면,
  - probability는 사건의 확률, likelihood는 사건의 가능도를 의미합니다. Likelihood를 계산할 때는 여러 사건들이 각각 독립이라고 가정하고, 하나의 사건에 대한 probability에 해당하는 parameter를 추정하기 위해 사용됩니다.
  - 이를 추정하는 방법은 통계학에서 MLE와 MAP가 있습니다. (MAP를 계산하기 위해서도 Likelihood가 필요함.)
  - 빈도주의자들은 관측된 데이터를 바탕으로 추정하는 MLE를, 단, 관측데이터는 모집단을 잘 표현할 수 있어야하며, 그렇지 않으면 overfitting이 일어납니다.
  - 베이지안주의자들은, 추정하려는 값이(여기서는 문맥상 확률?), 어떠한 사전분포(prior)를 따른다는 믿음을 바탕으로 관측된 데이터를 바탕으로 추정을 조금씩 개선해나가는(?) 방법입니다.
  - 이 경우 데이터의 수가 작은 경우, 사전분포에 영향을 받으며, 데이터가 많아질수록 MLE와 유사한 결과를 나오는 방향으로 값이 계산됩니다.
  - 그리고 이글은 neural network의 역할을 다음과 같이 본다고 생각했습니다.
  - 관찰(입력데이터) => neural network: approximation of $f(X \mid w)$ => $\theta$ => exponential family => probability of y 
  - 뒤에서 이틀에 대해 자세히 리뷰하겠지만, MLE의 관점에서 neural network가 예측한 $\theta$를 가능도로 해석하고, 각각의 곱이 최대가 되도록, 많이 곱해질수록 0에 수렴하기 때문에, log를 씌우고, loss를 최소화한다는 관점에서 -1을 곱하고, 최소화를 시키면, MLE의 컨셉이 Negative loglikelihood를 최소화는 문제와 동일해짐을 알 수 있다. 즉, NLL컨셉은 MLE와 동일하며 NN은, binary classification에서, pmf를 근사하는 function의 역할을 함. 
  - Entropy, 
