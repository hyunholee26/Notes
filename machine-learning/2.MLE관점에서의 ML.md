Uncertainty quatification을 위한 지표의 하나로 Negative Log Likelihood가 사용된다는 것이 잘 이해가 되지 않아
다음 블로그를 일게되 었는데, 정리가 필요하다고 생각하여 아래와 같이 정리한다.
 - Uncertainty란 모델이 예측결과를 얼마나 확신하는지 나타내는 지표이다.
 - https://jaejunyoo.blogspot.com/2018/02/minimizing-negative-log-likelihood-in-kor.html
 - https://jaejunyoo.blogspot.com/2018/02/inimizing-negative-log-likelihood-in-kor-2.html
 - https://jaejunyoo.blogspot.com/2018/02/minimizing-negative-log-likelihood-in-kor-3.html

### 0.목적
 - 이 글은 최종적으로 neural network에서 minimizaing the negative log likelihood의 의미를 설명합니다.

### 1. 1, 2편에서는 아래의 개념들을 간단히 설명합니다.
  - Random variable, Probability distribution
  - (정리차원에서), probability와 likelihood에 대한 설명을 덧붙이면,
  - probability는 사건의 확률, likelihood는 사건의 가능도를 의미합니다. Likelihood를 계산할 때는 여러 사건들이 각각 독립이라고 가정하고, 하나의 사건에 대한 probability에 해당하는 parameter를 추정하기 위해 사용됩니다.
  - 이를 추정하는 방법은 통계학에서 MLE와 MAP가 있습니다. (MAP를 계산하기 위해서도 Likelihood가 필요함.)
  - 빈도주의자들은 관측된 데이터를 바탕으로 추정하는 MLE를, 단, 관측데이터는 모집단을 잘 표현할 수 있어야하며, 그렇지 않으면 overfitting이 일어납니다.
  - 베이지안주의자들은, 추정하려는 값이(여기서는 문맥상 확률?), 어떠한 사전분포(prior)를 따른다는 믿음을 바탕으로 관측된 데이터를 바탕으로 추정을 조금씩 개선해나가는(?) 방법입니다.
  - 이 경우 데이터의 수가 작은 경우, 사전분포에 영향을 받으며, 데이터가 많아질수록 MLE와 유사한 결과를 나오는 방향으로 값이 계산됩니다.
  - 그리고 이글은 neural network의 역할을 다음과 같이 본다고 생각했습니다.
  - 관찰(입력데이터) => neural network: approximation of $f(X \mid w)$ => $\theta$ => exponential family => probability of y 
  - 뒤에서 이틀에 대해 자세히 리뷰하겠지만, MLE의 관점에서 neural network가 예측한 $\theta$를 가능도로 해석하고, 각각의 곱이 최대가 되도록, 많이 곱해질수록 0에 수렴하기 때문에, log를 씌우고, loss를 최소화한다는 관점에서 -1을 곱하고, 최소화를 시키면, MLE의 컨셉이 Negative loglikelihood를 최소화는 문제와 동일해짐을 알 수 있다. 즉, NLL컨셉은 MLE와 동일하며 NN은, binary classification에서, pmf를 근사하는 function의 역할을 함. 그리고 그 데이터를 잘 표현할 수록, NLL값은 작아짐. 그래서 NLL이 작아지도록 loss function을 설계함. 즉 작은 NLL은 데이터 distributino을 더 잘 표현? 한다고 해석할 수 있음.
  - Entropy: 평균정보량을 계산, 잘 일어나지 않는 사건은 자주 발생하는 사건보다 정보량이 많다. 모델의 예측결과에 대한 확률분포를 엔트로피로 계산하는 경우, 모델이 결과를 확신? 하는 경우 엔트로피가 낮으며, 모델이 결과를 확신하지 못하는 경우 엔트로피가 높다. 확률분포의 분산이 작은 경우, 엔트로피가 낮으며, 분산이 큰 경우 엔트로피가 높다.
  - supervised learning의 문제는 출력변수에 따라, regression, binary classification, categorical classification으로 구분되는데, 앞으로는 binary classification에 대해서만 집중하여 작성 예정
  - Maximun entropy distribution이라는 개념을 꺼내는 데, 이는 binary classification에서 해당 클래스의 확률 분포들이 가질 수 있는 최대 엔트로피 값과 최소한 같거나 큽니다. 가장 최소한의 정보만을 사용하여(largest entorpy) 분포를 정하는 것임.
  - binary classification의 경우, maximun entropy distribution은 binomial distribution이라고 함. (why?)
  - 이렇게 선택된 binomial distribution을 exponential family distribution의 꼴($P(y;n) = b(y) exp(n^T T(y) - a(n))$)로 변형을 하면,
  - $n = log(\frac{\phi}{1-\phi})$, canonical parameter
  - $T(y) = y$
  - $a(n) = -log(1-\phi)$, 분포를 정규화하는 데 사용됨
  - $b(y) = 1$
  - T,a,b를 고정하면, 확률분포의 결과는 n을 매개변수로 하는 함수가 됨
  - n은 $n = log(\frac{\phi}{1-\phi})$ 표현되지만, \phi를 매개변수를 가지는 함수형태, 이를 다시, $n = \theta^T x$로 표현하고, $\theta$를 추정하는 문제로 볼 수 있음
  - 따라서, $\phi = \frac{1}{1+e^{-n}}$의 관계를 가지며, $n = \theta^T x$로 표현할 수 있지만, NN의 output으로도 해석할 수 있음.
  - 이렇게, 뉴럴넷의 결과를 각 문제에 맞게 확률값으로 변환하는 것이 sigmoid나 softmax를 쓰는게 자연스럽다?라는 관점을 설명하며 1편을 마무리합니다.
  - (결론) binary classfication에서 뉴럴넷의 출력을 sigmoid로 변환하는 것은 Maximun entropy distribution 이론적 배경이 있다!
 
### 2. 3편 loss function, MLE, NLL에 대해 설명합니다.
  - y를 고정하고, parameter 값들이 바뀌도록 설정한다면, 같은 함수가 likelihood function이 되며, 이 함수는 고정된 y값에 대해 현재의 parameter의 likelihood에 대해 알려주는 것이 됨 (MLE는 파라메터 추정을 위해 사용되는 방식임, MLE를 만드는 $\theta$를 찾는 것임, 통계에서는 미분으로 찾지만, NN은 gradient descent를 이용해 parameter를 최적화함
  - 즉 loss function은 MLE의 형태를 띄어야함! MLE와 NLL은 동치임!
  - binomial distribution에 대해 NLL을 구하면, binary cross entropy를 최소화하는 것과 동치임.
  - NN의 $\theta$를 추정하는데 있어, prior를 N(0, V)로 걸경우, 이는 L-2 regularization(weight-decay)과 동치임, prior의 분산 V와 scaling constant가 관련됨.
  - NLL의 의미는 MLE와 동일하며, 그 값이 각각 최소, 최대화 될 수록 더 좋은 파라메터를 estimate했다고 볼 수 있으며, 이는 NN의 모델이 얼마나 잘(?) 학습되었는지를 볼 수 있다.
  - (MAP와 posterior distribution의 차이점 공부하기!) 
